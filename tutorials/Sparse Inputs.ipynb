{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "numpy.random.seed(0)\n",
    "\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "\n",
    "from apricot import FeatureBasedSelection\n",
    "from apricot import FacilityLocationSelection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Inputs\n",
    "\n",
    "Sometimes your data has many zeroes in it. Sparse matrices, implemented through `scipy.sparse`, are a way of storing only those values that are non-zero. This can be an extremely efficient way to represent massive data sets that mostly have zero values, such as sentences that are featurized using the presence of n-grams. Simple modifications can be made to many algorithms to operate on the sparse representations of these data sets, enabling compute to be efficiently performed on data whose dense representation may not even fit in memory. The submodular optimization algorithms implemented in apricot are some such algorithms.\n",
    "\n",
    "Let's start off with loading three data sets in scikit-learn that have many zeros in them, and show the density, which is the percentage of non-zero elements in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('digits density: ', 0.5107122982749026)\n",
      "('covtype density: ', 0.21997936040589205)\n",
      "('rcv1 density: ', 0.0015492886781268524)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "\n",
    "X_digits = load_digits().data.astype('float64')\n",
    "X_covtype = numpy.abs(fetch_covtype().data).astype('float64')\n",
    "X_rcv1 = fetch_rcv1().data[:5000].toarray()\n",
    "\n",
    "print(\"digits density: \", (X_digits != 0).mean())\n",
    "print(\"covtype density: \", (X_covtype != 0).mean())\n",
    "print(\"rcv1 density: \", (X_rcv1 != 0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these three data sets have very different levels of sparsity. The digits data set is approximately half non-zeros, the covertypes data set is approximately one-fifth non-zeroes, and the rcv1 subset we're using is less than 0.2% non-zeroes.\n",
    "\n",
    "Let's see how long it takes to rank the digits data set using only naive greedy selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 563 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit FeatureBasedSelection(X_digits.shape[0], 'sqrt', X_digits.shape[0]).fit(X_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn our dense numpy array into a sparse array using `scipy.sparse.csr_matrix`. Currently, apricot only accepts `csr` formatted sparse matrices. This creates a matrix where each row is stored contiguously, rather than each column being stored contiguously. This is helpful for us because each row corresponds to an example in our data set. No other changes are needed other than passing in a `csr_matrix` rather than a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 502 ms per loop\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_digits_sparse = csr_matrix(X_digits)\n",
    "\n",
    "%timeit FeatureBasedSelection(X_digits.shape[0], 'sqrt', X_digits.shape[0]).fit(X_digits_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like it had a huge effect on speed, though this is likely due to a comination of the data set being small and not particularly sparse.\n",
    "\n",
    "Let's look at the covertypes data set, which is both much larger and much sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:12<00:00, 39.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apricot.featureBased.FeatureBasedSelection at 0x7fe8ce4697d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sr_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c0c0e05bbe5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_covtype_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_covtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFeatureBasedSelection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_covtype_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sr_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "X_covtype_sparse = sr_matrix(X_covtype)\n",
    "\n",
    "FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X_covtype_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it might only be a little bit beneficial in terms of speed, here.\n",
    "\n",
    "Let's take a look at our last data set, the subset from rcv1, which is extremely sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X_rcv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rcv1_sparse = csr_matrix(X_rcv1)\n",
    "\n",
    "FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X_rcv1_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like there is a massive speed improvement here. It looks like the sparseness of a data set may contribute to the speed improvements one would get when using a sparse array versus a dense array. \n",
    "\n",
    "As a side note, only a small subset of the rcv1 data set is used here because, while the sparse array does fit in memory, the dense array does not. This illustrates that, even when there isn't a significant speed advantage, support for sparse arrays in general can be necessary for massive data problems. For example, here's an example of apricot easily finding the least redundant subset of size 10 from the entire 804,414 example x 47,236 feature rcv1 data set, which would require >250 GB to store at 64-bit floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rcv1_sparse = fetch_rcv1().data\n",
    "\n",
    "FeatureBasedSelection(10000, 'sqrt', 100, verbose=True).fit(X_rcv1_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there seems to be a speed benefit as data sets become larger. But can we quantify it further? Let's look at a large, randomly generated sparse data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(0)\n",
    "X = numpy.random.choice(2, size=(8000, 4000), p=[0.99, 0.01]).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X).ranking[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse = csr_matrix(X)\n",
    "\n",
    "FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X_sparse).ranking[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks much faster to use a sparse matrix for this data set. But, is it faster to use a sparse matrix because the data set is larger, or because we're leveraging the format of a sparse matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sizes = 500, 750, 1000, 1500, 2000, 3000, 5000, 7500, 10000, 15000, 20000, 30000, 50000 \n",
    "times, sparse_times = [], []\n",
    "for n in sizes:\n",
    "    X = numpy.random.choice(2, size=(n, 4000), p=[0.99, 0.01]).astype('float64')\n",
    "    \n",
    "    tic = time.time()\n",
    "    FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X)\n",
    "    times.append(time.time() - tic)\n",
    "    \n",
    "    X = csr_matrix(X)\n",
    "\n",
    "    tic = time.time()\n",
    "    FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X)\n",
    "    sparse_times.append(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = numpy.array(times) / numpy.array(sparse_times)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Sparse and Dense Timings\", fontsize=14)\n",
    "plt.plot(times, label=\"Dense Time\")\n",
    "plt.plot(sparse_times, label=\"Sparse Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(sizes)), sizes, rotation=45)\n",
    "plt.xlabel(\"Number of Examples\", fontsize=12)\n",
    "plt.ylabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Speed Improvement of Sparse Array\", fontsize=14)\n",
    "plt.plot(ratio)\n",
    "plt.xticks(range(len(sizes)), sizes, rotation=45)\n",
    "plt.xlabel(\"Number of Examples\", fontsize=12)\n",
    "plt.ylabel(\"Dense Time / Sparse Time\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like, at a fixed sparsity, the larger the data set is, the larger the speed up is.\n",
    "\n",
    "What happens if we vary the number of features in a data set with a fixed number of examples and sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = 5, 10, 25, 50, 100, 150, 200, 250, 500, 1000, 2000, 5000, 10000, 15000, 20000, 25000\n",
    "times, sparse_times = [], []\n",
    "for d in sizes:\n",
    "    X = numpy.random.choice(2, size=(10000, d), p=[0.99, 0.01]).astype('float64')\n",
    "    \n",
    "    tic = time.time()\n",
    "    FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X)\n",
    "    times.append(time.time() - tic)\n",
    "    \n",
    "    X = csr_matrix(X)\n",
    "\n",
    "    tic = time.time()\n",
    "    FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X)\n",
    "    sparse_times.append(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = numpy.array(times) / numpy.array(sparse_times)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Sparse and Dense Timings\", fontsize=14)\n",
    "plt.plot(times, label=\"Dense Time\")\n",
    "plt.plot(sparse_times, label=\"Sparse Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(sizes)), sizes, rotation=45)\n",
    "plt.xlabel(\"Number of Examples\", fontsize=12)\n",
    "plt.ylabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Speed Improvement of Sparse Array\", fontsize=14)\n",
    "plt.plot(ratio, label=\"Dense Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(sizes)), sizes, rotation=45)\n",
    "plt.xlabel(\"Number of Examples\", fontsize=12)\n",
    "plt.ylabel(\"Dense Time / Sparse Time\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're getting a similar speed improvement as we increase the number of features.\n",
    "\n",
    "Lastly, what happens when we change the sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 0.995, 0.999\n",
    "times, sparse_times = [], []\n",
    "for p in ps:\n",
    "    X = numpy.random.choice(2, size=(10000, 500), p=[p, 1-p]).astype('float64')\n",
    "    \n",
    "    tic = time.time()\n",
    "    FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X)\n",
    "    times.append(time.time() - tic)\n",
    "    \n",
    "    X = csr_matrix(X)\n",
    "\n",
    "    tic = time.time()\n",
    "    FeatureBasedSelection(500, 'sqrt', 500, verbose=True).fit(X)\n",
    "    sparse_times.append(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = numpy.array(times) / numpy.array(sparse_times)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Sparse and Dense Timings\", fontsize=14)\n",
    "plt.plot(times, label=\"Dense Time\")\n",
    "plt.plot(sparse_times, label=\"Sparse Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(ps)), ps, rotation=45)\n",
    "plt.xlabel(\"% Sparsity\", fontsize=12)\n",
    "plt.ylabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Speed Improvement of Sparse Array\", fontsize=14)\n",
    "plt.plot(ratio, label=\"Dense Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(ps)), ps, rotation=45)\n",
    "plt.xlabel(\"% Sparsity\", fontsize=12)\n",
    "plt.ylabel(\"Dense Time / Sparse Time\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it may be the most informative plot. This says that, given data sets of the same size, operating on a sparse array will be significantly slower than a dense array until the data set gets to a certain sparsity level. For this data set it was approximately 75% zeros, but for other data sets it may differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples have so far focused on the time it takes to select using feature based functions. However, facility location functions can take sparse input, as long as it is the pre-computed similarity matrix that is sparse, not the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = numpy.random.uniform(0, 1, size=(6000, 6000))\n",
    "X = (X + X.T) / 2.\n",
    "X[X < 0.9] = 0.0\n",
    "X_sparse = csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FacilityLocationSelection(500, 'precomputed', 500, verbose=True).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FacilityLocationSelection(500, 'precomputed', 500, verbose=True).fit(X_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks selection works significantly faster on a sparse array than on a dense one. We can do a similar type of analysis as before to analyze the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = range(500, 8001, 500)\n",
    "times, sparse_times = [], []\n",
    "for d in sizes:\n",
    "    X = numpy.random.uniform(0, 1, size=(d, d)).astype('float64')\n",
    "    X = (X + X.T) / 2\n",
    "    X[X <= 0.9] = 0\n",
    "    \n",
    "    tic = time.time()\n",
    "    FacilityLocationSelection(500, 'precomputed', 500, verbose=True).fit(X)\n",
    "    times.append(time.time() - tic)\n",
    "    \n",
    "    X = csr_matrix(X)\n",
    "\n",
    "    tic = time.time()\n",
    "    FacilityLocationSelection(500, 'precomputed', 500, verbose=True).fit(X)\n",
    "    sparse_times.append(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = numpy.array(times) / numpy.array(sparse_times)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Sparse and Dense Timings\", fontsize=14)\n",
    "plt.plot(times, label=\"Dense Time\")\n",
    "plt.plot(sparse_times, label=\"Sparse Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(sizes)), sizes, rotation=45)\n",
    "plt.xlabel(\"Number of Examples\", fontsize=12)\n",
    "plt.ylabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Speed Improvement of Sparse Array\", fontsize=14)\n",
    "plt.plot(ratio, label=\"Dense Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(sizes)), sizes, rotation=45)\n",
    "plt.xlabel(\"Number of Examples\", fontsize=12)\n",
    "plt.ylabel(\"Dense Time / Sparse Time\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 0.995, 0.999\n",
    "times, sparse_times = [], []\n",
    "for p in ps:\n",
    "    X = numpy.random.uniform(0, 1, size=(2000, 2000)).astype('float64')\n",
    "    X = (X + X.T) / 2\n",
    "    X[X <= p] = 0\n",
    "    \n",
    "    tic = time.time()\n",
    "    FacilityLocationSelection(500, 'precomputed', 500, verbose=True).fit(X)\n",
    "    times.append(time.time() - tic)\n",
    "    \n",
    "    X = csr_matrix(X)\n",
    "\n",
    "    tic = time.time()\n",
    "    FacilityLocationSelection(500, 'precomputed', 500, verbose=True).fit(X)\n",
    "    sparse_times.append(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = numpy.array(times) / numpy.array(sparse_times)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Sparse and Dense Timings\", fontsize=14)\n",
    "plt.plot(times, label=\"Dense Time\")\n",
    "plt.plot(sparse_times, label=\"Sparse Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(ps)), ps, rotation=45)\n",
    "plt.xlabel(\"% Sparsity\", fontsize=12)\n",
    "plt.ylabel(\"Time (s)\", fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Speed Improvement of Sparse Array\", fontsize=14)\n",
    "plt.plot(ratio, label=\"Dense Time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(range(len(ps)), ps, rotation=45)\n",
    "plt.xlabel(\"% Sparsity\", fontsize=12)\n",
    "plt.ylabel(\"Dense Time / Sparse Time\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to feature based selection, using a sparse array is only faster than a dense array when the array gets to a certain level of sparsity, but can then be significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
