{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Functions\n",
    "\n",
    "Although apricot has several feature-based and graph-based submodular functions implemented, the full space of submodular functions is too large to be stored on GitHub. Fortunately, for those who want to optimize their own functions, the apricot makes it easy to define custom functions and optimize them using the built-in optimizers. \n",
    "\n",
    "This tutorial will cover two ways of optimizing custom functions using apricot: (1) writing a function that simply returns the quality of any set of examples that is passed in and wrapping it with a custom selection objection, or (3) writing a class in the style of the existing selectors. The first approach is likely faster to implement but slower to execute, because a key aspect of the selection objects is using cached statistics to speed up calculations. Regardless of the approach taken, the existing optimizers can be applied to either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with the simplest way to optimize a custom function: by defining a set function and passing it in to `CustomSelection` or `CustomGraphSelection`. These selectors encode all the functionality needed for performing selection, and are initialized in the same way as the built-in selector objects, except that the user also passes in the function you want optimized. \n",
    "\n",
    "The custom function should take in a matrix where each row is an example in the selected set and each column is an attribute about that example. If the function is feature-based, each column should be a feature (and `CustomSelection` should be used); if the function is graph-based, each column should be the similarity to an example in the ground set and there should be a number of columns equal to the number of elements in the ground set (and `CustomGraphSelection` should be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature-Based Selection\n",
    "\n",
    "The `CustomSelection` object serves as a wrapper around a feature-based function. A function is \"feature-based\" when it relies on the feature values of an example rather than the similarity with other examples. The practical importance of this distinction is that, usually, feature-based functions can be optimized efficiently and scale to massive data sets because they do not require storing a similarity matrix that is quadratic with the number of examples.\n",
    "\n",
    "The built-in `FeatureBasedSelection` and `MaxCoverageSelection` objects wrap examples of feature-based functions. To demonstrate that the custom selection object works, let's implement a custom version of the function implemented in `FeatureBasedSelection` and compare it against the built-in object.\n",
    "\n",
    "Here is the submodular function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_feature_based_function(X):\n",
    "    return numpy.sqrt(X.sum(axis=0)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap our function using the `CustomSelection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import CustomSelection\n",
    "\n",
    "model = CustomSelection(100, sqrt_feature_based_function, optimizer='naive', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a data set of random non-negative values to apply our method to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.exp(numpy.random.randn(10000, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare against the built-in version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, -2.0747847884194925e-12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apricot import FeatureBasedSelection\n",
    "\n",
    "model = CustomSelection(100, sqrt_feature_based_function, optimizer='naive')\n",
    "model.fit(X)\n",
    "\n",
    "model0 = FeatureBasedSelection(100, 'sqrt', optimizer='naive')\n",
    "model0.fit(X)\n",
    "\n",
    "numpy.all(model.ranking == model0.ranking), (model.gains - model0.gains).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It looks like both implementations are selecting the same examples and reporting the same gain.   \n",
    "\n",
    "Now, let's make sure that we get the same subset even when we use a more complicated optimization method (be sure to set the same random state when using an optimizer that uses random values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, -3.979039320256561e-13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomSelection(100, sqrt_feature_based_function, optimizer='stochastic', random_state=0)\n",
    "model.fit(X)\n",
    "\n",
    "model0 = FeatureBasedSelection(100, 'sqrt', optimizer='stochastic', random_state=0)\n",
    "model0.fit(X)\n",
    "\n",
    "numpy.all(model.ranking == model0.ranking), (model.gains - model0.gains).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're getting the same subset even with a stochastic optimizer.\n",
    "\n",
    "Now, let's look at timing. How much faster is the built-in feature-based function selection than the custom selector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.56 s ± 40.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.26 s ± 5.43 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model1 = CustomSelection(100, sqrt_feature_based_function, optimizer='naive')\n",
    "model2 = FeatureBasedSelection(100, 'sqrt', optimizer='naive')\n",
    "\n",
    "%timeit model1.fit(X)\n",
    "%timeit model2.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the built-in implementation is much faster than our custom function implementation. In more thorough evaluations not shown here, it takes roughly 1s for `FeatureBasedSelection` to get initialized, and that increasing the number of selected examples or the size of the data set does not have much of an effect. In contrast, the `CustomSelection` object scales linearly with the number of examples selected and with the size of the data set. \n",
    "\n",
    "Although the previous example focused on recreating the existing feature-based selector, the real benefit of using the `CustomSelector` is that any submodular function can be passed in and optimized. Let's build a weird function that is a mixture of two step functions. To ensure that this function is submodular and that optimization is proceeding reasonably, let's compare the results we get from using the naive greedy algorithm to the lazy greedy algorithm. When the underlying function is not submodular the two will diverge, with the naive greedy algorithm producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weird_function(X):\n",
    "    X1, X2 = X > 1, X < 2\n",
    "    return numpy.sqrt((X * X1).sum(axis=0)).sum() + numpy.log1p((X2 * X).sum(axis=0)).sum() \n",
    "\n",
    "model1 = CustomSelection(100, weird_function, optimizer='naive')\n",
    "model2 = CustomSelection(100, weird_function, optimizer='lazy')\n",
    "\n",
    "model1.fit(X)\n",
    "model2.fit(X)\n",
    "\n",
    "numpy.all(model1.ranking == model2.ranking), (model1.gains - model2.gains).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Both optimizers are producing the same results even though they are optimizing this weird function.\n",
    "\n",
    "One can also conveniently pass in parameters to the custom function as a dictionary to the `function_kwds` parameter in the selector. This dictionary gets passed to the function as keyword arguments, and so the keys in the dictionary should correspond to the parameter names in the custom function that is passed in.  \n",
    "\n",
    "Let's see the use of passing parameters through the selector in the context of making a custom function that is a mixture of two objectives with weights that can be tuned. We can compare the results of this to the built-in mixture selector that offers the same functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, -8.752998326144734e-13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apricot import MixtureSelection\n",
    "\n",
    "def mixture_function(X, w0=1.0, w1=1.0):\n",
    "    X_sum = X.sum(axis=0)\n",
    "    return w0 * numpy.sqrt(X_sum).sum() + w1 * numpy.log1p(X_sum).sum()\n",
    "\n",
    "model1 = CustomSelection(100, mixture_function, function_kwds={'w0': 0.3, 'w1': 1.8}, optimizer='naive')\n",
    "model2 = MixtureSelection(100, [FeatureBasedSelection(100, 'sqrt'), FeatureBasedSelection(100, 'log')],\n",
    "                        weights=[0.3, 1.8])\n",
    "\n",
    "model1.fit(X)\n",
    "model2.fit(X)\n",
    "\n",
    "numpy.all(model1.ranking == model2.ranking), (model1.gains - model2.gains).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Graph-Based Selection\n",
    "\n",
    "Arguably graph-based functions, such as facility location and graph cut, are much more popular in the literature. These functions differ from feature-based functions in that the optimization proceeds over similarities between examples rather than the feature-values themselves. Indeed, one need not even have featurized examples at all: for instance, if your examples are locations and the \"similarity\" is derived from the physical distance between the locations, there is no need to even have underlying features (though for this example the underlying features might be the long/lat coordinates of the locations) (See Example B: Airport Selection for a more in-depth example).\n",
    "\n",
    "Our graph-based function should be implemented in the same general manner as a feature-based function. The function will take in a set of potential selected items and return the quality of that set. However, this function will assume that the columns on this data set will be the similarities between the example and all examples in the ground set. For example, here is the facility location objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facility_location_function(X):\n",
    "    return X.max(axis=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do is pop this function into a `CustomGraphSelection` object. Note that we do not need to specify a method for turning data sets into similarity matrices. In the same way that the built-in graph-based functions can handle this conversion internally (and have a parameter `metric` that defines how similarities are calculated), the `CustomGraphSelection` object can also handle this conversion internally. If you want to pass in a precomputed similarity matrix, you can specify `metric='precomputed'`.\n",
    "\n",
    "Here, we will use the lazy optimizer because it provides massive speed boosts over the naive optimizer on graph-based functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import CustomGraphSelection\n",
    "\n",
    "model = CustomGraphSelection(100, facility_location_function, optimizer='lazy', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare against the built-in facility location selector to ensure the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, -1.8474111129762605e-13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apricot import FacilityLocationSelection\n",
    "\n",
    "model = CustomGraphSelection(100, facility_location_function, metric='corr', optimizer='lazy')\n",
    "model.fit(X)\n",
    "\n",
    "model0 = FacilityLocationSelection(100, metric='corr', optimizer='lazy')\n",
    "model0.fit(X)\n",
    "\n",
    "numpy.all(model.ranking == model0.ranking), (model.gains - model0.gains).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the same cool functionality we showed for feature-based functions with `CustomSelection` is also possible with graph-based functions and `CustomGraphSelection`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own selection objects\n",
    "\n",
    "Although the `CustomSelection` and `CustomGraphSelection` wrappers are simple to use, they are likely much slower than using the built-in selectors. A reason for this is that the functions themselves are sped up using numba, making them significantly faster and multi-thread parallel. However, even if users wrote their own numba functions and wrapped them in the selection wrappers (which is certainly possible), the built-in selectors would likely still be faster. This is because the built-in selectors are able to cache function-specific statistics about the stored value at each iteration which can be used to speed up the calculation. \n",
    "\n",
    "For example, evaluating a feature-based function involve calculating the sum of each column of the selected set, applying a concave function, and then summing the result. However, if we already knew the sum of each column from the selected set in the previous iteration, we could speed up the computation significantly by simply adding the new element to these stored values element-wise.\n",
    "\n",
    "Likewise, evaluating a facility-location function involves calculating the nearest selected example for each example in the ground set. If you cache the similarity between each example in the ground set and its nearest selected example, you simply need to see whether the proposed newest item is more similar than the currently stored most-similar example using an element-wise max operation.\n",
    "\n",
    "Because the wrapped custom functions only evaluate the quality of a set, they do not even know which items were ultimately selected. Unfortunately, the set of statistics that would be valuable to cache depends on the objective function itself, and so there is no single set of statistics that could be passed into any function.\n",
    "\n",
    "If the wrapped selectors are not fast enough, one may choose to implement their own selector from scratch. Going this route allows the user to both define the function that they want to optimize as well as the statistics that should be cached.\n",
    "\n",
    "Below is an example of the basic skeleton that needs to be filled in to define an entire selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import BaseSelection\n",
    "\n",
    "class SkeletonSelection(BaseSelection):\n",
    "    # If you're defining a feature-based function you should inherit from BaseSelection\n",
    "    # If you're defining a graph-based function and want to make use of the built-in\n",
    "    # similarity calculations, you should inherit from BaseGraphSelection, which itself\n",
    "    # inherits from BaseSelection.\n",
    "    \n",
    "    def __init__(self, n_samples, initial_subset=None, optimizer='two-stage', \n",
    "        optimizer_kwds={}, n_jobs=1, random_state=None, \n",
    "        verbose=False):\n",
    "        \n",
    "        # If defining a graph-based function, you should also include a metric\n",
    "        # parameter, with the default in apricot being \"metric='euclidean'\".\n",
    "        # You will also need to pass the metric into the call to `super` below\n",
    "        # by adding \"metric=metric\" anywhere.\n",
    "        \n",
    "        # This function must call `__init__` of the base class, as below. \n",
    "        # Other custom arguments, such as hyperparameters for the underlying\n",
    "        # submodular function, should be stored in this method. \n",
    "        \n",
    "        super(SkeletonSelection, self).__init__(n_samples=n_samples, \n",
    "            initial_subset=initial_subset, optimizer=optimizer, \n",
    "            optimizer_kwds=optimizer_kwds, n_jobs=n_jobs, random_state=random_state, \n",
    "            verbose=verbose)\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        # This function includes any logic that should be executed before the\n",
    "        # selection process begins, such as initializing cached values and\n",
    "        # setting them to values from the initial subset.\n",
    "        \n",
    "        super(SkeletonSelection, self)._initialize(X)\n",
    "\n",
    "    def _calculate_gains(self, X, idxs=None):\n",
    "        # This function returns the gain in the objective function that each\n",
    "        # item in X[idxs] would return. The returned vector should be of\n",
    "        # length len(idxs) or len(self.idxs) if idxs is None. The value of\n",
    "        # gains[i] should be the gain associated with adding X[idxs[i]] to\n",
    "        # the selected set.\n",
    "\n",
    "        idxs = idxs if idxs is not None else self.idxs\n",
    "        gains = numpy.zeros(idxs.shape[0], dtype='float64')\n",
    "        return gains\n",
    "\n",
    "    def _select_next(self, X, gain, idx):\n",
    "        # This function takes in a single example, X, the corresponding gain\n",
    "        # of adding this element to the selected set, and the index of this\n",
    "        # example in the full data set, and updates the cached values accordingly.\n",
    "\n",
    "        super(SkeletonSelection, self)._select_next(X, gain, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the methods in the above class must be filled out, but likely the most important method is `_calculate_gains`. Notably, this method DOES NOT simply return the quality of some subset `X`, as the functions we wrapped before did. Rather, this method returns the gain that each element in `X[idxs]` would provide if it were added as the next item. Thus, each iteration of selection using the naive optimizer involves a single call to `_calculate_gains` followed by choosing the element with the largest gain. This calculates the gains for only those items indexes in `idxs` because there is no need to calculate the gain for elements that have already been selected, and also because some optimization strategies involve sampling only a portion of the data set to evaluate.\n",
    "\n",
    "Let's see an example of a simple implementation of a feature-based function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import BaseSelection\n",
    "\n",
    "class SqrtFeatureBasedSelector(BaseSelection):\n",
    "    '''This is a minimal implementation of a feature-based function with a sqrt.'''\n",
    "    \n",
    "    def __init__(self, n_samples, initial_subset=None, optimizer='two-stage', \n",
    "        optimizer_kwds={}, n_jobs=1, random_state=None, \n",
    "        verbose=False):\n",
    "        \n",
    "        super(SqrtFeatureBasedSelector, self).__init__(n_samples=n_samples, \n",
    "            initial_subset=initial_subset, optimizer=optimizer, \n",
    "            optimizer_kwds=optimizer_kwds, n_jobs=n_jobs, random_state=random_state, \n",
    "            verbose=verbose)\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        # The cached values will be the column sums.\n",
    "        self.current_values = numpy.zeros(X.shape[1])\n",
    "        \n",
    "        # We should also keep track of the total gain thus far\n",
    "        # so we can calculate the marginal gain of adding each\n",
    "        # element quickly.\n",
    "        self.total_gain = 0.0\n",
    "        super(SqrtFeatureBasedSelector, self)._initialize(X)\n",
    "\n",
    "    def _calculate_gains(self, X, idxs=None):\n",
    "        # The gains are the increase in the objective. This can be\n",
    "        # calculated as the objective value of each example minus\n",
    "        # the stored accumulated gain. Given that this is trivially\n",
    "        # vectorizable, the code is not actually complex.\n",
    "\n",
    "        idxs = idxs if idxs is not None else self.idxs\n",
    "        gains = numpy.sqrt(X[idxs] + self.current_values).sum(axis=1) - self.total_gain\n",
    "        return gains\n",
    "\n",
    "    def _select_next(self, X, gain, idx):\n",
    "        # Because we are storing column sums we only need to do an\n",
    "        # element-wise addition to update the cached values and\n",
    "        # another addition to store the accumulated gain.\n",
    "        \n",
    "        self.current_values += X\n",
    "        self.total_gain += gain\n",
    "        super(SqrtFeatureBasedSelector, self)._select_next(X, gain, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results from this custom object to the built-in feature-based selector and to the wrapped custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9403 9550 3391 1786 9858 5697 1647 5221 5379 1053] \n",
      " [136.62059032  69.77395573  52.10092935  43.88411145  39.33852143\n",
      "  35.93244774  32.49506686  30.3090869   27.89198382  25.90773747] \n",
      "\n",
      "[9403 9550 3391 1786 9858 5697 1647 5221 5379 1053] \n",
      " [136.62059032  69.77395573  52.10092935  43.88411145  39.33852143\n",
      "  35.93244774  32.49506686  30.3090869   27.89198382  25.90773747] \n",
      "\n",
      "[9403 9550 3391 1786 9858 5697 1647 5221 5379 1053] \n",
      " [136.62059032  69.77395573  52.10092935  43.88411145  39.33852143\n",
      "  35.93244774  32.49506686  30.3090869   27.89198382  25.90773747] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model0 = FeatureBasedSelection(10, 'sqrt', optimizer='naive')\n",
    "model0.fit(X)\n",
    "\n",
    "model1 = CustomSelection(10, sqrt_feature_based_function, optimizer='naive')\n",
    "model1.fit(X)\n",
    "\n",
    "model2 = SqrtFeatureBasedSelector(10, optimizer='naive')\n",
    "model2.fit(X)\n",
    "\n",
    "print(model0.ranking, \"\\n\", model0.gains, \"\\n\")\n",
    "print(model1.ranking, \"\\n\", model1.gains, \"\\n\")\n",
    "print(model2.ranking, \"\\n\", model2.gains, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news, it looks like all of them select the same elements and calculate the same gains for each one. \n",
    "\n",
    "Now, let's compare timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 s ± 5.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "7.6 s ± 49.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "486 ms ± 1.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model0 = FeatureBasedSelection(100, 'sqrt', optimizer='naive')\n",
    "%timeit model0.fit(X)\n",
    "\n",
    "model1 = CustomSelection(100, sqrt_feature_based_function, optimizer='naive')\n",
    "%timeit model1.fit(X)\n",
    "\n",
    "model2 = SqrtFeatureBasedSelector(100, optimizer='naive')\n",
    "%timeit model2.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, the wrapper for custom functions is far slower than the built-in selector. Interestingly, it seems like the custom feature-based method we just implemented might be faster than the built-in selector. Does this hold up with larger data sets, or is it only the case for the somewhat small toy data set we were using throughout this notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.8 s ± 692 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "X = numpy.exp(numpy.random.randn(100000, 500))\n",
    "\n",
    "model0 = FeatureBasedSelection(1000, 'sqrt', optimizer='naive')\n",
    "%timeit model0.fit(X)\n",
    "\n",
    "model2 = SqrtFeatureBasedSelector(1000, optimizer='naive')\n",
    "%timeit model2.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the built-in selector has a small startup cost (around 1s), as we saw earlier in this tutorial, but ends up scaling better to larger problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "In this notebook we saw how one can use apricot to optimize custom functions in two convenient ways. The simplest way is to define a function that evaluates a set and wrap it with either `CustomSelection` or `CustomGraphSelection`, but an alternative that potentially can be significantly faster is to define an entire selection object. Either approach gives a user access to the wide variety of features implemented in apricot, such as optimizing their function using any predefined optimization algorithm.\n",
    "\n",
    "Although the functions that we optimized here were entirely submodular, in keeping with the focus of apricot, the custom functions need not necessarily be. Recent work has suggested that supermodular functions can approximately optimized well, in practice, using the same naive greedy algorithm that are used to optimize submodular function. The naive greedy algorithm will no longer have the same theoretical guarantees when applied to supermodular functions instead of submodular functions, but usage in apricot is the same: define a custom function and wrap it, or define a selector that caches statistics to speed up computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
