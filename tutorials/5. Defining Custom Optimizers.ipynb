{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom Optimizers\n",
    "\n",
    "Currently, apricot implements a range of different optimization approaches. In the same way that any optimizer in neural network libraries can be applied to any model, any optimizer in apricot can be applied to any submodular function and thus is agnostic to the details about it. This property is convenient when it comes to defining custom optimizers, because it means that one can focus entirely on the definition of the optimizer and then use it on any built-in (or even custom) function in apricot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeleton of an optimizer object is simple, and has the following form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot.optimizers import BaseOptimizer\n",
    "\n",
    "class SkeletonOptimizer(BaseOptimizer):\n",
    "    # The optimizer object should always inherit from BaseOptimizer\n",
    "    # or an object that inherits from BaseOptimizer.\n",
    "\n",
    "    def __init__(self, function=None, random_state=None, n_jobs=None, \n",
    "        verbose=False):\n",
    "        # The optimzier object should take in the above parameters\n",
    "        # and pass them in to the super function. Any optimizer-specific\n",
    "        # hyperparameters should also be passed in here.\n",
    "        \n",
    "        super(SkeletonOptimizer, self).__init__(function=function, \n",
    "            random_state=random_state, n_jobs=n_jobs, verbose=verbose)\n",
    "\n",
    "    def select(self, X, k, sample_cost=None):\n",
    "        # This is the key function. The `select` method should be called a\n",
    "        # single time and returns either a subset of size k or a subset with\n",
    "        # a weighted sum of less than or equal to k. \n",
    "        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select` method is where the optimization algorithm is implemented. It uses the `_calculate_gains` and `_select_next` methods in the selection objects to determine the benefit of adding each element in and inform the selector of the next item to add to the set, respectively.\n",
    "\n",
    "Let's implement a new version of the naive greedy algorithm. This algorithm, at each iteration, calculates the gain that each element would provide if added to the subset, and then selects the item with the best gain. Due to the diminishing returns property of submodular functions, it is not guaranteed that the second best item at any particular iteration will be the best item the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNaiveGreedy(BaseOptimizer):\n",
    "    def __init__(self, function=None, random_state=None, n_jobs=None, \n",
    "        verbose=False):\n",
    "        # The naive greedy algorithm has no hyperparameters.\n",
    "        super(SimpleNaiveGreedy, self).__init__(function=function, \n",
    "            random_state=random_state, n_jobs=n_jobs, verbose=verbose)\n",
    "\n",
    "    def select(self, X, k, sample_cost=None):\n",
    "        # This is a version of the naive greedy algorithm that ignores\n",
    "        # sample weights, for demonstration purposes.\n",
    "        \n",
    "        for i in range(k):\n",
    "            gains = self.function._calculate_gains(X)\n",
    "            idx = numpy.argmax(gains)\n",
    "            best_idx = self.function.idxs[idx]\n",
    "\n",
    "            self.function._select_next(X[best_idx], gains[idx], best_idx)\n",
    "\n",
    "            if self.verbose == True:\n",
    "                self.function.pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple! All we had to do is get the gains of each element, get the element with the largest gain (and index it properly), and then tell the selector which item was the best! Obviously, not all optimization strategies are this straightforward. For instance, the stochastic greedy algorithm involves evaluating subsets of points at each iteration and choosing the best item from different subsets at each iteration. Control over the precise elements being evaluated is important for that optimizer.\n",
    "\n",
    "How do we use this custom optimizer, though? Well, we just pass it in to the `optimizer` parameter that each selection has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import FeatureBasedSelection\n",
    "\n",
    "model1 = FeatureBasedSelection(100, 'sqrt', optimizer='naive')\n",
    "model2 = FeatureBasedSelection(100, 'sqrt', optimizer=SimpleNaiveGreedy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's run it and make sure we get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = numpy.exp(numpy.random.randn(10000, 100))\n",
    "\n",
    "model1.fit(X)\n",
    "model2.fit(X)\n",
    "\n",
    "numpy.all(model1.ranking == model2.ranking), (model1.gains - model2.gains).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Do they take the same amount of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 s ± 9.92 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.21 s ± 9.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model1.fit(X)\n",
    "%timeit model2.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like they do. That makes sense because most of the expensive calculation is being done within the selection object. The only thing being done here is taking an argmax of a vector, which is basically what the built-in optimizer is doing anyway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
