{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Tutorial 1: Practical Apricot Usage\n",
    "\n",
    "Welcome to a tutorial on how to reduce redundancy in massive data sets using submodular optimization! In this tutorial, we will explore submodular optimization at a high level and see how it can be used to select representative subsets of data; these subsets can then be used on their own, such as to create a highlight reel for an album, or to create smaller training sets for machine learning models that achieve similar performance in a fraction of the time. Although submodular optimization is as general-purpose as convex optimization, this tutorial will focus on using basic optimization algorithms on two main functions: a feature-based function, and facility location functions. Finally, this tutorial will focus on practical usage of apricot. Please see the other tutorials for more of the theory behind how these functions work. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import seaborn\n",
    "seaborn.set_style('whitegrid')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-based Selection\n",
    "\n",
    "A simple class of submodular functions are the feature-based ones. At a high level, feature-based functions are those that maximize diversity in the observed feature values themselves. This property means that they work well in settings where each feature represents some quality of the data and higher values mean that the example has more of that value: for instance, when vectorizing text data, each feature might represent a word and the value would be the number of times that the word appears in the document.\n",
    "\n",
    "More formally, feature-based functions take the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\sum\\limits_{u \\in U} w_{u} \\phi_{u} \\left( \\sum\\limits_{x \\in X} m_{u}(x) \\right)\n",
    "\\end{equation}\n",
    "where $x$ is a single example, $X$ is the set of all examples, $u$ is a single feature, $U$ is the set of all features, $w$ is a weight foe each feature, and $\\phi$ is a saturating concave function such as log or sqrt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 newsgroups\n",
    "\n",
    "Let's start off with some text data. Below, some code is provided to download a shuffled version of the 20 newsgroups data set, which contains articles and labels for 20 topics. However, as we can see, the downloaded text is not in a convenient featurized form that can be used by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', random_state=0, shuffle=True)\n",
    "train_data.data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing this to get rid of the weird characters like \"\\n\" and converting it to a vectorized form is not really the point of this tutorial, so let's use sklearn's built-in vectorizer to get a clean feature matrix to operate on. Please fill in the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = ...\n",
    "\n",
    "X_train = ...\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how dense that data is. We can do this by creating a heatmap where each red dot represents that the feature hasa non-zero value. If you implemented the above code correctly you should get a density of 0.08395."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000\n",
    "\n",
    "X_random_block = X_train[:i].toarray()\n",
    "random_density = (X_random_block != 0).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(*numpy.where(X_random_block.T[:i] != 0), c='r', s=0.05)\n",
    "plt.xlim(0, i)\n",
    "plt.ylim(0, i)\n",
    "\n",
    "plt.title(\"Words in Text Blobs: Density={:4.4}\".format(random_density), fontsize=14)\n",
    "plt.xlabel(\"Word Index\", fontsize=12)\n",
    "plt.ylabel(\"Text Blob Index\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heat map is made up of the first 1000 entries in the data set after shuffling. It doesn't seem particularly dense; fewer than 10% of the values in the matrix are positive. This may not be particularly problematic when restricting to 1000 features, but since more features generally means more accuracy in this setting, is there a way to ensure that our subset sees a higher percentage of the features?\n",
    "\n",
    "Well, choosing examples that exhibit values in a diverse set of features is exactly what submodular optimization and feature based functions are good at. We can define a feature-based function easily using apricot, choose an equal sized subset of examples using submodular optimization, and re-visualize the chosen examples.\n",
    "\n",
    "Fill in the next code block, using a feature-based selector to choose 1000 samples with everything else set to the default parameters. You can also set `verbose=True` to see a progress bar during selection. Note that, while apricot can operate on sparse matrices, you might need to use the `toarray()` method to convert a sparse array to a dense array for the subsequent visualization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import FeatureBasedSelection\n",
    "\n",
    "selector = ...\n",
    "\n",
    "X_submodular_block = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've selected the examples, we can visualize the block in the same way that we visualized the randomly selected examples. If you implemented the selector correctly, you should get a density of 0.2103. Visually, the heatmap should also look significantly more red. This is because we are intentionally choosing examples that have many non-zero values, i.e., rows that would have red in a lot of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodular_density = (X_submodular_block != 0).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(*numpy.where(X_submodular_block.T[:i] != 0), c='r', s=0.05)\n",
    "plt.xlim(0, i)\n",
    "plt.ylim(0, i)\n",
    "\n",
    "plt.title(\"Words in Text Blobs: Density={:4.4}\".format(submodular_density), fontsize=14)\n",
    "plt.xlabel(\"Word Index\", fontsize=12)\n",
    "plt.ylabel(\"Text Blob Index\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can take a look at the number of words that are observed at least once as we select more and more examples, either randomly, or using submodular optimization. If your implementation of selecting a subset of examples using apricot is correct you should see that a larger number of words are observed earlier in the selection process when submodular optimization is used. You do not need to do anything here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seen_words = (numpy.cumsum(X_random_block, axis=0) > 0).sum(axis=1)\n",
    "submodular_seen_words = (numpy.cumsum(X_submodular_block, axis=0) > 0).sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"# Total Uniques Words Seen\", fontsize=14)\n",
    "plt.plot(random_seen_words, color='0.5', linewidth=2, label=\"Random\")\n",
    "plt.plot(submodular_seen_words, color='#FF6600', linewidth=2, label=\"Submodular Optimization\")\n",
    "plt.xlabel(\"# Examples Chosen\", fontsize=12)\n",
    "plt.ylabel(\"# Words Seen At Least Once\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"# New Words Seen per Example\", fontsize=14)\n",
    "plt.plot(numpy.diff(random_seen_words), color='0.5', linewidth=2, label=\"Random\")\n",
    "plt.plot(numpy.diff(submodular_seen_words), color='#FF6600', linewidth=2, label=\"Submodular Optimization\")\n",
    "plt.xlabel(\"# Examples Chosen\", fontsize=12)\n",
    "plt.ylabel(\"# New Words in Example\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to move on to the primary goal of apricot: choosing subsets for training machine learning models. Unfortunately, this is not always straightforward. As an example, we are going to consider classifying a subset of classes from the 20 newsgroups data set. Here are the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_20newsgroups(subset=\"train\").target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an initial example, we will focus on two classes that are somewhat related conceptually but will likely have distinct vocabularies. We will use the TF-IDF vectorizer instead of the count vectorizer because TF-IDF is a straightforward way to downweight words that appear in many articles and to upweight words that are somewhat rare and more likely to be topic-specific. Please fill in the below code involving processing the training and test data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "categories = ['sci.med', 'sci.space']\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, random_state=0, shuffle=True)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, random_state=0)\n",
    "\n",
    "vectorizer = ...\n",
    "\n",
    "X_train = ...\n",
    "X_test = ...\n",
    "\n",
    "y_train = ...\n",
    "y_test = ...\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use a feature-based function to select 1000 examples from the training data. 1000 examples is almost all of the data, but because the selection process is greedy we can use it to rank most of the data and then choose increasingly large subsets to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a SGG classifier on subsets of increasing size and compare to ten draws of similarly sized random subsets. Please fill in the below code, keeping in mind that the `selector.ranking` attribute contains a ranking of indices from the original data set. For example, if the first element was `10`, that would mean that `X_train[10]` was the first element chosen by the greedy optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(random_state=0)\n",
    "counts = numpy.arange(10, 1001, 10)\n",
    "\n",
    "random_idxs = numpy.array([numpy.random.choice(X_train.shape[0], replace=False, size=1000) for i in range(10)])\n",
    "random_accuracies, submodular_accuracies = [], []\n",
    "\n",
    "for count in tqdm(counts):\n",
    "    idxs = selector.ranking[:count]\n",
    "    \n",
    "    ...\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = (y_hat == y_test).mean()\n",
    "    submodular_accuracies.append(acc)\n",
    "    \n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        r_idxs = random_idxs[i, :count]\n",
    "        \n",
    "        ...\n",
    "\n",
    "        y_hat = model.predict(X_test)\n",
    "        acc = (y_hat == y_test).mean()\n",
    "        accs.append(acc)\n",
    "    \n",
    "    random_accuracies.append(accs)\n",
    "\n",
    "plt.title(\"20 Newsgroups Classification\", fontsize=14)\n",
    "plt.plot(counts, numpy.mean(random_accuracies, axis=1), color='0.5', linewidth=2, label=\"Random\")\n",
    "plt.plot(counts, submodular_accuracies, color='#FF6600', linewidth=2, label=\"Submodular Optimization\")\n",
    "\n",
    "plt.xlabel(\"# Chosen Examples\", fontsize=12)\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.legend(loc=4, fontsize=12)\n",
    "\n",
    "seaborn.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we can get almost the same performance with just 100 examples (~93% with submodular optimization and ~85% with random selection) as we could with the full set of 1000 examples. It is worth noting that there is a lot of variance when the number of examples chosen is very small, but that performance picks up pretty quickly. If you're not seeing these trends, it's possible that you implemented something incorrectly.\n",
    "\n",
    "If you'd like to explore apricot's abilities more broadly, try out the above cells using different sets of categories from the 20 newsgroups corpus and different types of classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Warning: Gaussian Blobs\n",
    "\n",
    "Unfortunately, not all data is amenable to feature-based functions. Specifically, data sets where the features don't follow the semantics that are assumed, i.e., non-negative and a higher value conveys some notion of having \"more\" of some feature. If you have features like coordinates or embeddings from a pre-trained model or projections from a method like tSNE or UMAP, they may not work as you'd like.\n",
    "\n",
    "Here, we will look at using data drawn from random Gaussian blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "numpy.random.seed(0)\n",
    "centers = numpy.random.normal(100, 5, (5, 2))\n",
    "X, y = make_blobs(n_samples=2500, n_features=2, centers=centers, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will fill in how to use a selector for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureBasedSelection(n_samples=100)\n",
    "selector.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply the selector and get our representative subset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian Blob Data\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Feature Based Selected Examples\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*X[selector.ranking].T, color='#FF6600', s=10, label=\"Selected Examples\")\n",
    "plt.axis('off')\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops. That doesn't look like a representative subset.\n",
    "\n",
    "Does this mean that feature-based functions cannot work in settings where the data doesn't have the same semantics as our assumptions? No! We just need to engineer features that do follow those semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Feature-based Functions: Gaussian Mixture Models\n",
    "\n",
    "Potentially, one of the most straightforward ways to transform this Gaussian data would be to, first, apply a Gaussian mixture model to it, and second, use the posterior probabilities from that model as the features. Basically, instead of applying submodular optimization to the original feature values themselves, we apply them to the predicted class probabilities from the mixture model. These probabilities have all the properties that we would like: (1) because they are between zero and one they must be non-negative, (2) a higher value means an enrichment for that feature, i.e., a higher probability means an enrichment for class membership from that class.\n",
    "\n",
    "Using the `GaussianMixture` object below, transform the above data from in `X` from the original feature values into the posterior probabilities. Because the data was generated from five clusters, your mixture should have five components. If done correctly, the resulting shape should be `(2500, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model = ...\n",
    "X_posteriors = ...\n",
    "X_posteriors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply a feature-based selector as you've done in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureBasedSelection(n_samples=100)\n",
    "selector.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the mixture centroids as well as the selected examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian Blob Data\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Feature Based Selected Examples\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10, label=\"GMM Centroids\")\n",
    "plt.scatter(*X[selector.ranking].T, color='#FF6600', s=10, label=\"Selected Examples\")\n",
    "plt.axis('off')\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look like what you might expect?\n",
    "\n",
    "If not, think more closely about the feature-based function and the data here. The sum of each example should be equal to one, so there are no examples that have a higher coverage of the feature space than other examples. However, the feature-based function includes a saturation function that diminishes the benefit of high values in one feature versus spreading them out across several features. Combined, these facts mean that the method will always try to choose examples that are split between multiple classes. Put another way, `numpy.sqrt([0.5, 0.5]).sum() = 1.414` is larger than `numpy.sqrt([1.0, 0]).sum() = 1.0`.\n",
    "\n",
    "Regardless of the explanation, this isn't exactly what we were expecting. What we'd like to do is have a way that our feature-based function can select examples near the middle of each cluster without needing cluster labels. The problem with using the posteriors, which are normalized to sum to 1, is that examples that are purest for a particular cluster are not the ones closest to the centroid but rather the ones that are on the other side of the centroid as all the other centroids. \n",
    "\n",
    "What does that mean? Well, let's use a simple trick to try to pick out the purest examples from each cluster. First, we need to transform these values such that values near one become bigger, so that purity is valued higher, but values near zero remain the same. We can use an `arctanh` function for that, but you should try out any other function you'd like to see the effects. Below is an example `arctanh` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.arange(0, 1, 0.001)\n",
    "plt.plot(x, numpy.arctanh(x))\n",
    "plt.title(\"Example Non-Linear Function\", fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=12)\n",
    "plt.ylabel(\"arctanh(x)\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arctanh = numpy.arctanh(X_posteriors - 1e-12) + 1e-12\n",
    "\n",
    "selector = FeatureBasedSelection(n_samples=100)\n",
    "selector.fit(X_arctanh)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian Blob Data\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Feature Based Selected Examples\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10, label=\"GMM Centroids\")\n",
    "plt.scatter(*X[selector.ranking].T, color='#FF6600', s=10, label=\"Selected Examples\")\n",
    "plt.axis('off')\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some interesting trends here. Unlike the previous plot where all the chosen examples were near boundaries, most of the chosen examples are on very edge of the convex hull. A notable exception, however, is the top cluster. This is likely because the top cluster is so far away from the others that any example in it is considered \"pure.\" \n",
    "\n",
    "Finally, let's get to the expected behavior. We would like to design a transformation such that our selection chooses elements that are neat representations of each cluster individually. We saw previously that using the normalized posterior probabilities can be an issue because the normalization process encourages the chosen examples to be far away from the other centroids, rather than close to any particular centroid. If we get rid of that normalization process and instead use the raw probabilities that each example belongs to a particular mixture component, we can get around this.\n",
    "\n",
    "In the cell below, use the `multivariate_normal` method from scipy to calculate an array of probabilities for each example under each mixture component. Hint: you will need to do this separately for each component as part of a loop. Make sure that your final output is of shape `(n_samples, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "X_probs = ...\n",
    "\n",
    "selector = FeatureBasedSelection(n_samples=100)\n",
    "selector.fit(X_probs)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian Blob Data\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Feature Based Selected Examples\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.5', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10, label=\"GMM Centroids\")\n",
    "plt.scatter(*X[selector.ranking].T, color='#FF6600', s=10, label=\"Selected Examples\")\n",
    "plt.axis('off')\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done this correctly, you'll notice that all of the chosen examples are near the centroids of the clusters. \n",
    "\n",
    "At this point, you might be wondering \"why do I need submodular optimization to do this?\" because you can just take the examples closest to centroids. The answer is two-fold: first, submodular optimization can be applied to any type of transformation where it may not be obvious how to do it by hand. Second, submodular optimization automatically balances the number of examples chosen per centroid based on their distance. This isn't a particularly complicated task here where all of the clusters are distinct, but consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(0)\n",
    "centers = numpy.random.normal(100, 4, (5, 2))\n",
    "X2, _ = make_blobs(n_samples=1000, n_features=2, centers=centers, cluster_std=3, random_state=0)\n",
    "\n",
    "model2 = GaussianMixture(5, random_state=0).fit(X2)\n",
    "X_probs = numpy.array([multivariate_normal.pdf(X2, model2.means_[i], model2.covariances_[i]) for i in range(5)]).T\n",
    "\n",
    "selector = FeatureBasedSelection(n_samples=100)\n",
    "selector.fit(X_probs)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian Blob Data\", fontsize=14)\n",
    "plt.scatter(*X2.T, color='0.5', s=10)\n",
    "plt.scatter(*model2.means_.T, color='b', s=10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Feature Based Selected Examples\", fontsize=14)\n",
    "plt.scatter(*X2.T, color='0.5', s=10)\n",
    "plt.scatter(*model2.means_.T, color='b', s=10, label=\"GMM Centroids\")\n",
    "plt.scatter(*X2[selector.ranking].T, color='#FF6600', s=10, label=\"Selected Examples\")\n",
    "plt.axis('off')\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the selected examples are close to one of the centroids, which is arguably in low-data areas and probably have a smaller variance. Because the other four centroids are in data-richer areas their variances likely overlap significantly, and so the chosen examples are in the central region between the three of them. Simply choosing the points near the centroid would not give the same results. This isn't to say that this is always exactly the most representative set from this data, just that this is a case where submodular optimization will provide different results from a simpler approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facility Location Selection\n",
    "\n",
    "An alternative to feature-based functions are graph-based functions. These functions operate on a similarity matrix (note: a similarity matrix is the inverse of a distance matrix, where the most similar elements in a distance matrix have a pairwise value of zero whereas the most distant elements in a similarity matrix have a pairwise value of zero) instead of the feature values directly. Graph-based functions are generally more versatile than feature-based ones because any featurization of data can be converted into a similarity graph by calculating the Euclidean distance or correlation between examples, but data types that are inherently graphs can also be operated on. \n",
    "\n",
    "A specific graph-based function is the facility location function, which has been used in the past to literally locate new facilities. In this setting, one wants to identify the next location that would serve the most people that are currently underserved without having to move any of the previous locations. The facility location function takes the following form: \n",
    "\n",
    "\\begin{equation}\n",
    "f(X, V) = \\sum\\limits_{v \\in V} \\max\\limits_{x \\in X} \\phi(x, v)\n",
    "\\end{equation}\n",
    "where $x$ is a selected example $X$ is the set of already selected examples, $v$ is an unselected example, $V$ is the set of unselected examples, and $\\phi$ is a similarity function that either returns an entry in a pre-defined similarity matrix or calculates the similarity between two examples.\n",
    "\n",
    "A challenge with using graph-based functions is that the similarity matrix has to be calculated and stored in memory for efficient computation, which can be challenging for massive data sets. However, it is more versatile because similarities can be calculated that are more informative than simple featurizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Version of K-means Clustering\n",
    "\n",
    "A simple way to think about using submodular optimization to optimize a facility location function is that it is a greedy version of k-medoids clustering. As a refresher, k-medoids clustering is similar to k-means except that the cluster centroids must be examples from the training set. It is similar to the difference between calculating the mean and calculating the median. Submodular optimization on a facility location function involves iteratively choosing the example that best explains the previously explained examples, i.e., that maximizes the increase in similarity between all of the examples and all of the chosen examples.\n",
    "\n",
    "What does that look like in practice? Implement a facility location selection object to choose 50 examples. You'll notice that, despite being a graph-based function, you can still pass in a feature matrix and it will automatically calculate a similarity graph from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot import FacilityLocationSelection\n",
    "\n",
    "selector = ...\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Gaussian Blob Data\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.7', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Facility Location Selected Examples\", fontsize=14)\n",
    "plt.scatter(*X.T, color='0.7', s=10)\n",
    "plt.scatter(*model.means_.T, color='b', s=10, label=\"GMM Centroids\")\n",
    "plt.scatter(*X[selector.ranking].T, color='#FF6600', s=10, label=\"Selected Examples\")\n",
    "plt.axis('off')\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected examples should appear to be fairly uniformly distributed across the space. If you're noticing a concentration of points anywhere, you may have incorrectly implemented something.\n",
    "\n",
    "To get a sense for the selection process, let's visualize the iterative process of selecting examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.title(\"{} Selections\".format(i+1), fontsize=14)\n",
    "    plt.scatter(*X.T, color='0.7', s=10)\n",
    "    \n",
    "    if i > 0:\n",
    "        plt.scatter(*X[selector.ranking[:i]].T, color='0.1', s=10, label=\"Selected Examples\")\n",
    "    \n",
    "    plt.scatter(*X[selector.ranking[i]].T, color='#FF6600', s=10, label=\"Next Selection\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.legend(loc=(1.01, 0.5), fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the first example comes from around the center of the data set. As a greedy approach, the optimizer is trying to find the single best example without knowing if it will be able to choose future ones. Then, the second example comes from an underrepresented area, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits Data Set\n",
    "\n",
    "Now, let's apply facility location functions to choosing data for machine learning. A constraint of the feature-based functions is that they only work when the features follow a particular set of semantics. Although there are powerful approaches for transforming features into new features that follow those semantics, it's also nice to not have to do anything fancy to get a good set of items. A good example of data where the assumptions of feature-based functions don't work out of the box are those that involve images.\n",
    "\n",
    "Let's download a reduced version of the digits data set and try training a machine learning model using selected subsets or random subsets, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "idxs = numpy.arange(X.shape[0])\n",
    "numpy.random.shuffle(idxs)\n",
    "\n",
    "X = X[idxs]\n",
    "y = y[idxs]\n",
    "\n",
    "X_train, y_train = X[:1000], y[:1000]\n",
    "X_test, y_test = X[1000:], y[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, impelment a facility location function to choose 1000 examples and a feature-based function to also choose 1000 examples, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_selector = ...\n",
    "\n",
    "fb_selector = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the subsets selected using facility location fare against those selected using random selection or feature-based selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(0)\n",
    "\n",
    "model = SGDClassifier(random_state=0)\n",
    "counts = numpy.arange(10, 1001, 10)\n",
    "\n",
    "random_idxs = numpy.array([numpy.random.choice(X_train.shape[0], replace=False, size=1000) for i in range(10)])\n",
    "random_accuracies, fl_accuracies, fb_accuracies = [], [], []\n",
    "\n",
    "for count in tqdm(counts):\n",
    "    #\n",
    "    idxs = ...\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = (y_hat == y_test).mean()\n",
    "    fl_accuracies.append(acc)\n",
    "\n",
    "    #\n",
    "    idxs = ...\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = (y_hat == y_test).mean()\n",
    "    fb_accuracies.append(acc)\n",
    "    \n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        r_idxs = random_idxs[i, :count]\n",
    "        ...\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        acc = (y_hat == y_test).mean()\n",
    "        accs.append(acc)\n",
    "    \n",
    "    random_accuracies.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Reduced MNIST Classification\", fontsize=14)\n",
    "plt.plot(counts, numpy.mean(random_accuracies, axis=1), color='0.5', linewidth=2, label=\"Random\")\n",
    "plt.plot(counts, fl_accuracies, color='#FF6600', linewidth=2, label=\"Facility Location Optimization\")\n",
    "plt.plot(counts, fb_accuracies, color='g', linewidth=2, label=\"Feature-Based Optimization\")\n",
    "\n",
    "plt.xlabel(\"# Chosen Examples\", fontsize=12)\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.legend(loc=4, fontsize=12)\n",
    "\n",
    "seaborn.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the facility location function achieves high accuracy with only a small number of examples! Using only 40 examples achieves almost 90% accuracy, whereas it takes almost 200 randomly selected examples to get hat performance on average. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
